---
title: "Survival Analysis of Factors Affecting Time on Market for Twin Cities Homes"
author: "Nick McMullen, April Choi"
date: "April 20, 2018"
output:
  pdf_document:
    pandoc_args: [
      "-V", "classoption=twocolumn" ]
  html_document: default
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readr)
library(ggplot2)
library(survival)
library(survminer)
library(stargazer)

house <- read_csv("../Source/house_TC.csv")
house <- house %>% mutate(tom = ifelse(tom == 0, 1, tom),
                          prdiff_np = as.factor(prdiff_np))
```

## 1 | Introduction


### 1.1 Background

The real estate market in the Twin Cities (Minneapolis and St. Paul, MN, USA), and many places across the United States, has been thriving on the economic recovery since the end of the Great Recession. While the current market is, to the casual observer, favorable to sellers some houses in Twin Cities still take a surprisingly long time to sell. We will investigate the characteristics that impact time on market (TOM) for houses in the Twin Cities through a survival analysis, or time-to-event, framework.


### 1.2 Review of Literature

\textbf{Price}

One of the most obvious factors that may influence TOM for any house is price. A high price may scare potential buyers away, while a low price might invite skepticism or a long bidding war. Moreover, a high price limits the number of potential buyers even if the price appropriately reflects the quality of the property. The "right" pricing decision is a difficult one to make for sellers and agents, but reaching an equilibrium between buyer and seller satisfaction may be the best way to minimize TOM. Cheng, Lin, and Liu (2008) developed a closed-form formula to uncover the theoretical relationship between price and TOM. They aim to describe the marginal benefit of keeping a house on the market longer.

The authors used house sales data from Fannie Mae and Freddie Mac, U.S. federal mortgage agencies, and identified a nonlinear positive relationship between price and TOM. They approached this analysis by assuming that a buyer and seller arrive at an agreed upon price following the Poisson processes at rate $\lambda$. While utilizing methods from Bond et al. (2007), they investiaged some assumptions about the shape of TOM data, including normal, chi-square, Weibull, and exponential distributions. Like Bond et al. (2007), Cheng, Lin, and Liu (2008) found that the exponential distribution best fit the TOM data. 

The findings from Cheng, Lin, and Liu's (2008) investigation was that marginal benefit on sale price decreases after each offer that is made on a house. As a house's price approaches equilibrium between seller and buyer satisfaction, it does the seller little good to leave the house on the market longer.

\textbf{Duration Dependence}

Another interesting question regarding TOM is how the probability of sale changes with time. Thomas W. Zuehlke (1987) conducted research on this very topic. He used 290 single family detached homes obtained from a 1982 multiple listing service (MLS) book in Tallahassee, FL, USA and observed how likely each house was to sell based on its vacancy status. Zuehlke utilized a Weibull hazard model to compare the two and found that sellers of vacant houses have stronger incentives to reduce prices faster than those of occupied houses. Thus, Zuehlke finds that vacant houses tend to exhibit positive duration dependence, while occupied homes show little evidence of duration dependence. 

Motivated by a Massachusetts state policy adopted in 2006 that prohibits sellers from resetting their home's TOM by relisting, Tucker, Zhang, and Zhu (2013) also investigated the impact of TOM on sale price. The authors agree that longer TOM is negatively associated with buyer perception, which lines up with the Massachusetts relisting policy. To investigate if TOM was indeed a significant deterrent for buyers looking at an otherwise appealing house, the authors analyzed the TOM for homes before and after the enactment of the policy. They obtained listings data for residential properties on the market between January 2005 and June 2007 from two MLSs: MLS-PIN, which serves Massachusetts, and the State-Wide Rhode Island MLS. Their analysis consisted of three groupings of homes: listed and sold before the policy change, listed before and sold after, and listed and sold after. The authors utilized simple linear regression between the three groups and found that the homes listed before the policy and sold after were most severely impacted, resulting in an average sale price reduction of $16,000. 

\textbf{Listing Price Changes}

Listing price changes may also affect the TOM for a house. A listing price change could attract more attention to a house on the market, but could also indicate a longer TOM. John R. Knight (2002) conducted an analysis on listing price changes, investigated which types of homes were most likely to go through multiple listing price changes, and which price changes tend to give the worst results for the seller.

Knight used 3490 detached single family homes that sold between January 1997 and December 1998 by Metroservices Inc. of Sacramento, CA, USA. He notes that price adjustment data is generally missing from TOM datasets, but his study incorporated the price changes into the analysis to examine the determinants of list price changes. He utilized a maximum likelihood probit model and found that the two most important determinants of price changes are TOM and initial markup. Atypical homes tend to not see significant price changes because there is little market precedent for price changes after a certain TOM. Knight's findings are consistent with previous research on pricing under demand uncertainty. 


### 1.3 Research Question (and mapping paragraph)

We are interested in investigating the factors that affect the TOM for a single family homes in the Twin Cities (Minneapolis and Saint Paul) that were sold between July 26, 2017 and February 1, 2018. Some variables of interest include the amount of recorded crime in a house's designated neighborhood and the proximity of a house to the nearest school. 

In the following sections, we outline our data collection and methods and provide a brief description of our assumptions. 

##2 | Data and Methods

### 2.1 Data and Sources

#### Real Estate Data

Our data represents a random sample of 311 single family homes in the Twin Cities that were sold between July 26th, 2017 and February 1st, 2018. A search of all recently sold homes in the area was conducted in the Coldwell Banker MLS and 311 unique addresses were randomly selected for the project. Some variables were scraped directly from the Coldwell Banker site, while others were manually inputted. 

#### Crime Data

We also obtained crime data beginning on January 1, 2015 from the government websites for the cities of Minneapolis and St. Paul. Each crime was matched with a neighborhood, aggregated into total neighborhood crime counts, and then joined with the house sales data. This addition will provide insight on the number of crimes that have been recorded over the past three years near each sold house. 

### 2.2 Variables

For all 311 observations, we collected unique addresses `address`, city of residence `city`, zip code `zip`, number of bedrooms `beds`, number of full bathrooms `bathf`, number of partial bathrooms `bathp`, number car garage `carg`, house squarefeet `sqft`, price the home was listed at `listedatpr`, price the home was sold for `soldatpr`, date the home was listed `listedatdate`, date the home was sold `soldatdate`, neighborhood the house is located `neighb`, distance to nearest school (mi.) `schdist`, and number of crimes that were recorded in that neighborhood beginning on January 1, 2015 `ncrime`. We also calculated the time on market `tom` for each house from list date to sale date, included the property ID `pid` from the Coldwell Banker website, calculated a price difference from list price to sale price `prdiff`, computed a percent change price `prdiff_perc`, and created a categorical variable `prdiff_np` to reflect whether a house had a positive (1), negative (2), or neutral (3) price difference. 

### 2.3 Assumptions

An essential assumption to our research is based on censoring. We intentionally collected our data from a sample of recently sold houses to limit censoring. If we were to take a true random sample of all houses listed for sale in the past $X$ number of years, we realize that a significant amount of right censoring could be present, because at the time of data collection some houses in our sample would have not been sold yet. We acknowledge that sampling from a set of recently sold houses may introduce some bias into our results - all recently sold houses in this particular date range (late January to early Februray 2018) may share certain unknown or unaccounted-for characteristics that made them sell. This bias is acceptable considering the convenience and added accuracy provided by uncensored data.

### 2.4 Methods

We utilize visualizations of parametric and non-parametric estimates of the true survival curve for `Time on Market (tom)` and test for significant differences in `tom` among houses with different characteristics, such as the price difference between list and sold price. 

##3 | Preliminary Results

We begin our preliminary analysis by investigating the length of time that houses in the Twin Cities remain on the market after their preliminary listing date.

### 3.1 Non-Parametric Survival Curve Estimates for Aggregate Data

#### Kaplan-Meier

First, we fit a Kaplan-Meier curve with confidence intervals for survival (time on market) across all groups. Since the data contains no censoring, Kaplan-Meier is equivalent to `1-ecdf` representative of the raw data itself. From the data, the mean is approximately 85.97 days, the median is 73 days, and the 95% CI is [66, 82]. The curve is right-skewed, indicating that there are more houses that sell faster than average. 

```{r echo=FALSE}

# mean(house$tom)
# median(house$tom)
KM = survfit( Surv(tom) ~ 1 , conf.type="plain" , data=house ) 
ggsurvplot(KM, data=house) +
  labs(x = "Days on Market", y = "Survival Probability", title = "Kaplan-Meier Curve of Days on Market")

```

HERE!!! (3KM models)



### 3.2 Univariate Models

#### School Distance 

```{r include=FALSE}
mw = survreg( Surv(tom) ~ schdist , dist = "weibull" , data = house )
mln = survreg( Surv(tom) ~ schdist , dist = "lognormal" , data = house )
mcox = coxph( Surv(tom) ~ schdist , data = house )

summary(mw) # the sign of schdist makes sense (positive) and it is significant
summary(mln) # the sign of schdist is negative and it is NOT significant
mcox # sign agrees with weibull and it is significant
```

The Weibull model indicates that the properties that are farther from schools have longer TOM than those closer to schools, with a significant p-value. However, the Log Normal model yields the opposite result with an insignificant p-value. This result may be due to the inherent differences in the two models, rather than the school distance variable being insignificant. In fact, the Cox PH model agrees with the Weibull model in the variable's direction and p-value. According to the Cox PH model, the hazard ratio of school distance is 0.701, which means that the risk of being sold decreases by 70% for every one-mile increase in school distance. In other words, properties farther away from schools take longer to sell.

```{r include = FALSE}
# Create Cox-Snell residuals
CS_w = -log( 1 - pweibull(house$tom, 1/0.537, exp(4.443 + 0.246*house$schdist) ) )
CS_ln = -log( 1 - plnorm(house$tom, 4.3010 + -0.0313*house$schdist, 0.623) ) 

# Make appropriate graph using CoxSnell function
# Creates Cox-Snell Plot:
CoxSnell = function(cs, status, xlim=NULL, ylim=NULL) {
  kmcs=survfit(Surv(jitter(cs,amount=(max(cs)-min(cs))/1000),status)~1)$surv
  plot(log(-log(kmcs))~sort(log(cs)),xlab="log(Cox-Snell)",ylab="log(-log(S(Cox-Snell)))",xlim=xlim,ylim=ylim)
  abline(0,1,col='red') }

house$status = rep(1, nrow(house))
CoxSnell( CS_w , house$status )
CoxSnell( CS_ln , house$status ,xlim=c(-7,5))
```

```{r include = FALSE}
2*(3-mw$loglik[2]) # mw
2*(3-mln$loglik[2]) # mln

# The AIC for the Weibull model is 3242.878, and that for the Log Normal model is 3258.564. Thus, we conclude that the Weibull model fits better than the Log Normal model. 
```

The Log Normal model seems to fit better than the Weibull model according to the Cox Snell residual plots, but AIC indicates that the Weibull model is a better fit.



#### Crime Categorical

```{r include=FALSE}
house$crimeCat2 = cut(house$ncrime, breaks = c(0,250,2000,5000,10000))

mw = survreg( Surv(tom) ~ crimeCat2, dist = "weibull" , data = house )
mln = survreg( Surv(tom) ~ crimeCat2, dist = "lognormal" , data = house )
mcox = coxph( Surv(tom) ~ crimeCat2, data = house )

summary(mw)
summary(mln)
mcox
```

Crime as a continuous variable was insignificant for all three models. Thus, we analyze the effect of crime as a categorical variable. Our categorical crime variable has four groups: crime count ranging from 0 to 250, 250 to 2000, 2000 to 5000, and 5000 to 10000.  

In the Weibull model, the 250-2000 crime group has TOM that is 14% longer than the 0-250 crime group, and the next group, the 2000-5000 crime group, has more than double the TOM of the previous group at 33%. While the p-value for 250-2000 crime group is at boarderline significance, that for the 2000-5000 crime group is significant at the 5% level. The fact that the houses in the neighborhoods with less crime (250-2000) would sell faster than those in the neighborhoods with more crime (2000-5000) makes sense since people generally prefer neighborhoods with less crime. 

On the other hand, the highest crime group (5000-10000) shows almost no difference in TOM compared to the lowest crime group, and its p-value is insignificant. This result is possibly due to the fact that properties located in high-crime neighborhoods tend to be more affordable. 

The Cox PH model agrees with the Weibull model. The hazard ratio for the 250-2000 crime group is approximately 19% less than that of the 0-250 crime group, and that for the (2000, 5000] group is approximately 36% less than that of the 0-250 crime group. Finally, the hazard ratio for the 5000-10000 crime group is only about 6% lower than the 0-250 crime group, again with insignificant p-value. The Log Normal model too is insignificant. 

```{r include = FALSE}
mw$coefficients

# Create Cox-Snell residuals
CS_w = -log( 1 - pweibull(house$tom, 1/0.533, exp(4.45486375 +  0.13794492*(house$crimeCat2=="(250,2e+03]") + .28846076*(house$crimeCat2=="(2e+03,5e+03]") +   0.03853525*(house$crimeCat2=="(5e+03,1e+04]")     ) ) )
CS_ln = -log( 1 - plnorm(house$tom, 4.3010 + -0.0313*house$schdist, 0.623) ) 

# Make appropriate graph using CoxSnell function
# Creates Cox-Snell Plot:
CoxSnell = function(cs, status, xlim=NULL, ylim=NULL) {
  kmcs=survfit(Surv(jitter(cs,amount=(max(cs)-min(cs))/1000),status)~1)$surv
  plot(log(-log(kmcs))~sort(log(cs)),xlab="log(Cox-Snell)",ylab="log(-log(S(Cox-Snell)))",xlim=xlim,ylim=ylim)
  abline(0,1,col='red') }

house$status = rep(1, nrow(house))
CoxSnell( CS_w , house$status )
CoxSnell( CS_ln , house$status )
```

```{r include = FALSE}
2*(3-mw$loglik[2]) # mw
2*(3-mln$loglik[2]) # mln
```

Again, the AIC indicates that the Weibull model is a better fit than the Log Normal model.

#### Listed Price

```{r}
mw = survreg( Surv(tom) ~ log(listedatpr), dist = "weibull" , data = house )
mln = survreg( Surv(tom) ~ log(listedatpr), dist = "lognormal" , data = house )
mcox = coxph( Surv(tom) ~ log(listedatpr), data = house )

summary(mw)
summary(mln)
mcox
```

```{r include = FALSE}
exp(0.120) #w
exp(0.189) #ln
```

Properties with higher listed price has a longer TOM at the 5% level significance for both Weibull and Log Normal models. The effect is stronger with the Log Normal model at 20% longer TOM with a much higher significance (4.20e-03) compared to 12% longer TOM for the Weibull model with a slightly lower significance (3.48e-02). The results are consistent with the logic that a more expensive homes will take longer to sell. Despite the greater significance with a stronger effect, the AIC still yields a stronger evidence in support of the Weibull model over the Log Normal model by a slight margin. (!!! too detailed?)

```{r include = FALSE}
2*(3-mw$loglik[2]) # mw
2*(3-mln$loglik[2]) # mln

# Again, the AIC indicates that the Weibull model is a better fit than the Log Normal model.
```


### 3.3 Multivariate Models

#### Crime Category 2 + Listed at Price + School Distance
#### Without Interaction Term (Original)

```{r echo=FALSE, include=FALSE}
mw = survreg( Surv(tom) ~ schdist + crimeCat2 + log(listedatpr) , dist = "weibull" , data = house )
mln = survreg( Surv(tom) ~ schdist + crimeCat2 + log(listedatpr) , dist = "lognormal" , data = house )
mcox = coxph(Surv(tom) ~ schdist + crimeCat2 + log(listedatpr) , data = house )


summary(mw) 
summary(mln) 
mcox
```

If the price triples (mult by 2.81 or exp(1)), the the chance of it selling gets multiplied by 0.742. (Victor)

Model Interpretations:

Weibull:

- School distance has a negative multiplicative effect on time on market by $e^{-.026}$ with a p-value of $.375$. This is not a significant result.

- Number of crimes in the neighborhood has a positive multiplicative effect on time on market of $e^{.00000649}$ with a p-value by $.0959$. This is also not a significant result.

- The date at which the house was listed has a negative multiplicative impact on the time on market by $e^{-.0105}$ with a p-value of $0$. This makes sense because as we progress one date forward, we are getting closer to the time at which a house was sold since we only included sold houses in our data. CHECK -- Is this necessary to include?

- The log of the price at which a house was listed has a positive multiplicative impact on the time on market by $e^{.0646}$ with a p-value of $.00191$. This is a significant result. This model shows that for every dollar a house's log list price increases, the time on market for that house is expected to increase by about $6.6732$%. 


Lognormal:

- School distance has a negative multiplicative effect on time on market by $e^{-.177}$ with a p-value of $.00087$. This is a significant result. This model shows that for every mile farther a house is away from the nearest school, the time on market is expected to decrease by about $16.332$%.

- Number of crimes in the neighborhood has a positive multiplicative effect on time on market of $e^{.0000124}$ with a p-value by $.0927$. This is not a significant result.

- The date at which the house was listed has a negative multiplicative impact on the time on market by $e^{-.0102}$ with a p-value of $0$. This makes sense because as we progress one date forward, we are getting closer to the time at which a house was sold since we only included sold houses in our data. CHECK -- Is this necessary to include?

- The log of the price at which a house was listed has a positive multiplicative impact on the time on market by $e^{.0702}$ with a p-value of $.05$. This is a significant result at the 5% level. This model shows that for every dollar a house's log list price increases, the time on market for that house is expected to increase by about $7.2723$%. 


*Hypotheses*

Now we must decide which model is better. To do this, we will first plot the Cox-Snell residuals for the Weibull and Lognormal models and then compare the AIC calculations for each model.

* Ho: weibull: tom ~ beds + bathf + bathp + carg + sqft + schdist + ncrime + prdiff
* Ha: lognorm: tom ~ beds + bathf + bathp + carg + sqft + schdist + ncrime + prdiff


*Cox-Snell Residuals*

Log-normal looks the best!

```{r echo=FALSE, include=FALSE}
# Creates Cox-Snell Plot:
CoxSnell = function(cs, status, xlim=NULL, ylim=NULL) {
  kmcs=survfit(Surv(jitter(cs,amount=(max(cs)-min(cs))/1000),status)~1)$surv
  plot(log(-log(kmcs))~sort(log(cs)),xlab="log(Cox-Snell)",ylab="log(-log(S(Cox-Snell)))",xlim=xlim,ylim=ylim)
  abline(0,1,col='red') }
```


```{r include=FALSE}

house$status <- rep(1, 311)

# Create Cox-Snell residuals
CS_e = -log( 1 - pexp(house$tom, 1/exp(4.153050e+00 + 5.260030e-02*house$beds + 4.625418e-02*house$bathf + 3.530967e-02*house$bathp + -4.039299e-02*house$carg -2.186615e-05*house$sqft + 1.252779e-01*house$schdist + 5.489057e-06*house$ncrime + -5.267278e-06*house$prdiff) ) )
CS_w = -log( 1 - pweibull(house$tom, 1/0.532846, exp(4.153050e+00 + 5.260030e-02*house$beds + 4.625418e-02*house$bathf + 3.530967e-02*house$bathp + -4.039299e-02*house$carg -2.186615e-05*house$sqft + 1.252779e-01*house$schdist + 5.489057e-06*house$ncrime + -5.267278e-06*house$prdiff ) ) )
CS_ln = -log( 1 - plnorm(house$tom, 4.153050e+00 + 5.260030e-02*house$beds + 4.625418e-02*house$bathf + 3.530967e-02*house$bathp + -4.039299e-02*house$carg -2.186615e-05*house$sqft + 1.252779e-01*house$schdist + 5.489057e-06*house$ncrime + -5.267278e-06*house$prdiff, 0.5914082) ) 

# Make appropriate graph using CoxSnell function
CoxSnell( CS_w , house$status )
CoxSnell( CS_ln , house$status )
```

The lognormal model appears to perform better when evaluating performance graphically.

*AIC*

Lognormal (3240.594) performs better in the AIC calculation than Weibull (3241.18). 

```{r echo=FALSE, include=FALSE}
mln$loglik

2*(10--1610.590) # mw
2*(10--1610.297) # mln
```


*Summary of Cox Model*
!!!

```{r}
library(survminer)
fit = coxph(Surv(tom) ~ schdist + crimeCat2 + log(listedatpr) , data = house )
fit
ggforest(fit, data = house)
```


```{r include = FALSE}
stargazer(house[c(9,14,22),])
stargazer(house[c(9,14,22),], summary=FALSE, rownames=FALSE)
stargazer(mw, mln, title="Results", align=TRUE)
stargazer(mcox, title="Results", align=TRUE)
```


### Full Summary Table

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 




### Simple Summary Table

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}} cccccccccccccccccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
address & city & zip & beds & bathf & bathp & carg & sqft & listedatpr & soldatpr & listedatdate & soldatdate & neighb & schdist & ncrime & pid & tom & prdiff & prdiff\_perc & prdiff\_np & status & crimeCat2 \\ 
\hline \\[-1.8ex] 
704 White Bear Avenue N & Saint Paul & 55106 & 2 & 1 & 1 & 1 & 1089 & 139000 & 138000 & 17447 & 17562 & Conway/Battlecreek/Highwood & 0.4 & 4453 & pid\_20224294/ & 115 & -1000 & -0.00719424460431655 & 2 & 1 & 3 \\ 
967 Fuller Avenue & Saint Paul & 55104 & 2 & 1 & 0 & 1 & 1560 & 199900 & 2e+05 & 17511 & 17562 & Summit/University & 0.3 & 4545 & pid\_21292535/ & 51 & 100 & 0.000500250125062531 & 1 & 1 & 3 \\ 
3830 Aldrich Avenue N & Minneapolis & 55412 & 3 & 1 & 1 & 1 & 1254 & 159900 & 159900 & 17503 & 17561 & Webber - Camden & 1 & 460 & pid\_21194229/ & 58 & 0 & 0 & 3 & 1 & 2 \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 




## Output: Weibull and Log Normal

\begin{table}[!htbp] \centering 
  \caption{Results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{tom} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{\textit{Weibull}} & \multicolumn{1}{c}{\textit{survreg: lognormal}} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}\\ 
\hline \\[-1.8ex] 
 schdist & 0.184^{**} & -0.045 \\ 
  & (0.090) & (0.105) \\ 
  & & \\ 
 crimeCat2(250,2e+03] & 0.188^{**} & 0.094 \\ 
  & (0.084) & (0.096) \\ 
  & & \\ 
 crimeCat2(2e+03,5e+03] & 0.252^{***} & 0.226^{**} \\ 
  & (0.087) & (0.097) \\ 
  & & \\ 
 crimeCat2(5e+03,1e+04] & 0.109 & 0.170 \\ 
  & (0.098) & (0.114) \\ 
  & & \\ 
 log(listedatpr) & 0.131^{**} & 0.228^{***} \\ 
  & (0.059) & (0.073) \\ 
  & & \\ 
 Constant & 2.712^{***} & 1.374 \\ 
  & (0.751) & (0.925) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{311} & \multicolumn{1}{c}{311} \\ 
Log Likelihood & \multicolumn{1}{c}{-1,611.420} & \multicolumn{1}{c}{-1,619.327} \\ 
$\chi^{2}$ (df = 5) & \multicolumn{1}{c}{22.426$^{***}$} & \multicolumn{1}{c}{13.995$^{**}$} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 




### Output: Cox PH

\begin{table}[!htbp] \centering 
  \caption{Results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & \multicolumn{1}{c}{tom} \\ 
\hline \\[-1.8ex] 
 schdist & -0.258 \\ 
  & (0.176) \\ 
  & \\ 
 crimeCat2(250,2e+03] & -0.327^{**} \\ 
  & (0.160) \\ 
  & \\ 
 crimeCat2(2e+03,5e+03] & -0.425^{**} \\ 
  & (0.166) \\ 
  & \\ 
 crimeCat2(5e+03,1e+04] & -0.237 \\ 
  & (0.186) \\ 
  & \\ 
 log(listedatpr) & -0.298^{***} \\ 
  & (0.114) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{311} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.057} \\ 
Max. Possible R$^{2}$ & \multicolumn{1}{c}{1.000} \\ 
Log Likelihood & \multicolumn{1}{c}{-1,468.756} \\ 
Wald Test & \multicolumn{1}{c}{17.950$^{***}$ (df = 5)} \\ 
LR Test & \multicolumn{1}{c}{18.218$^{***}$ (df = 5)} \\ 
Score (Logrank) Test & \multicolumn{1}{c}{18.002$^{***}$ (df = 5)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 






### 3.4 Testing the Cox PH Assumption

Not Worked on Yet













###### BELOW IS FIRST DRAFT WORK FOR REFERENCE ########

### 3.3 Non-Parametric Statistiscal Tests

#### List and Sale Price Difference

#### Log-Rank Test

In order to determine if the differences in the estimated survival curves for this category are statistically significant, we conduct a log-rank test. The results are as follows:

$$H_0: S_1(t) = S_2(t) = S_3(t) \ \forall \ t$$

The TOM distributions for the three price-difference conditions are identical at all time points.

$$H_A: S_1(t) \neq S_2(t) \neq S_3(t) \ \text{for some} \ t$$

The TOM distributions for the three price-difference conditions differ at at least one time point.

```{r echo=FALSE, include=FALSE}

survdiff(Surv(tom) ~ as.factor(prdiff_np), data=house)

```

A log-rank test reveals a Mantel-Cox test statistic of $53.87$. This produces a p-value of nearly 0:

```{r echo=FALSE}

1 - pchisq(37.71+10.75+5.41, df=2)

```

Thus, we confidently reject the null hypothesis and conclude that the three survival curves for price difference (negative, zero, positive) differ for some `tom`. 

#### Hazard Ratio

Now, we compare the risk of failures (being sold) between two groups at a time by examining the hazard ratios. The first comparison is between the high-demand and low-demand homes. The hazard ratio is

```{r echo=FALSE}
# Note: + = hot houses
(HR = (81/41.5)/(193/244.2)) # + / -
```

and the confidence interval is

```{r echo=FALSE}
cat("[", HR*exp(-1.96*sqrt(1/41.5+1/244.2)), ", ", HR*exp(+1.96*sqrt(1/41.5+1/244.2)),"]", sep="")
```

The hazard ratio indicates that the probability of high-demand houses being sold is 2.47 times higher than that for low-demand houses. The confidence interval suggests that this hazard estimate is significant since the interval does not bound 1.


Next, we see the difference between low-demand and neutral-demand homes. The hazard ratio is

```{r echo=FALSE}
(HR = (193/244.2)/(37/25.3)) # - / 0
```

and the confidence interval is

```{r echo=FALSE}
cat("[", HR*exp(-1.96*sqrt(1/244.2+1/25.3)), ", ", HR*exp(+1.96*sqrt(1/244.2+1/25.3)),"]", sep="")
```

Hazard ratio shows that low-demand homes have around half the chance of being sold on the market compared to neutral-demand homes. Since the confidence interval does not encompass 1, the hazard estimate is significant.


Last, we compare high-demand and neutral-demand homes. The hazard ratio is

```{r echo=FALSE}
# Note: + = hot houses
(HR = (81/41.5)/(37/25.3)) # + / 0
```

and the confidence interval is

```{r echo=FALSE}
cat("[", HR*exp(-1.96*sqrt(1/41.5+1/25.3)), ", ", HR*exp(+1.96*sqrt(1/41.5+1/25.3)),"]", sep="")
```

Despite the hazard ratio of 1.33, the interval estimate suggests that the this difference of 1.33 is insignificant. This result is inconsistent with the significant p-value we obtained from the log-rank test.

Examining the confidence intervals of the three groups from the Kaplan-Meier curves, we see that high-demand and equilibrium houses lay on top of each other almost completely until around 40 days, and the confidence intervals overlap until approximately 175 days, which explain the insignificance in the hazard ratio estimate. On the other hand, low-demand and high-demand homes are quite distinctive and separate from each other with only a few spots where the confidence intervals intersect. This distinct separation is also reflective of the large hazard ratio (2.47) with confidence interval way greater than 1 ([1.777065, 3.431999]). 
